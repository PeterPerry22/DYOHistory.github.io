(this["webpackJsonpDYOHistory.github.io"]=this["webpackJsonpDYOHistory.github.io"]||[]).push([[0],{34:function(e,t,i){},44:function(e,t,i){},45:function(e,t,i){},46:function(e,t,i){},65:function(e,t,i){"use strict";i.r(t);var a=i(1),n=i(27),s=i.n(n),o=(i(18),i(9)),r=i(2),c=(i(34),i(8)),l=i.n(c),h=i(0),d=function(){function e(){var e=l()("#navbarSupportedContent").find(".active"),t=e.innerHeight(),i=e.innerWidth(),a=e.position(),n=e.position();l()(".hori-selector").css({top:a.top+"px",left:n.left+"px",height:t+"px",width:i+"px"}),l()("#navbarSupportedContent").on("click","li",(function(e){l()("#navbarSupportedContent ul li").removeClass("active"),l()(this).addClass("active");var t=l()(this).innerHeight(),i=l()(this).innerWidth(),a=l()(this).position(),n=l()(this).position();l()(".hori-selector").css({top:a.top+"px",left:n.left+"px",height:t+"px",width:i+"px"})}))}return Object(a.useEffect)((function(){e(),l()(window).on("resize",(function(){setTimeout((function(){e()}),500)}))}),[]),Object(h.jsxs)("nav",{className:"navbar navbar-expand-lg navbar-mainbg",children:[Object(h.jsx)("button",{className:"navbar-toggler",onClick:function(){setTimeout((function(){e()}))},type:"button","data-toggle":"collapse","data-target":"#navbarSupportedContent","aria-controls":"navbarSupportedContent","aria-expanded":"false","aria-label":"Toggle navigation",children:Object(h.jsx)("i",{className:"fas fa-bars text-white"})}),Object(h.jsx)("div",{className:"collapse navbar-collapse",id:"navbarSupportedContent",children:Object(h.jsxs)("ul",{className:"navbar-nav ml-auto nav-fill w-100",children:[Object(h.jsxs)("div",{className:"hori-selector",children:[Object(h.jsx)("div",{className:"left"}),Object(h.jsx)("div",{className:"right"})]}),Object(h.jsx)("li",{className:"nav-item active",children:Object(h.jsxs)(o.b,{className:"nav-link",to:"/",exact:!0,children:[Object(h.jsx)("i",{className:"fas fa-tachometer-alt"}),"Anthology"]})}),Object(h.jsx)("li",{className:"nav-item",children:Object(h.jsxs)(o.b,{className:"nav-link",to:"/news",exact:!0,children:[Object(h.jsx)("i",{className:"far fa-address-book"}),"News"]})}),Object(h.jsx)("ui",{className:"navbar-logo-ui",children:Object(h.jsx)(o.b,{className:"navbar-logo",to:"/",exact:!0,children:"Modern Luddism"})}),Object(h.jsx)("li",{className:"nav-item",children:Object(h.jsxs)(o.b,{className:"nav-link",to:"/support",exact:!0,children:[Object(h.jsx)("i",{className:"far fa-clone"}),"Support"]})}),Object(h.jsx)("li",{className:"nav-item",children:Object(h.jsxs)(o.b,{className:"nav-link",to:"/profile",exact:!0,children:[Object(h.jsx)("i",{className:"far fa-copy"}),"Profile"]})})]})})]})},u=(i(44),i(29));var m=function(){return document.body.style="background: #7A00B2;",Object(h.jsx)("div",{className:"App-bg",children:Object(h.jsxs)("div",{id:"wrapper",children:[Object(h.jsx)("h1",{className:"main-title",children:"Welcome"}),Object(h.jsx)("div",{className:"Introduction",children:Object(h.jsx)("text",{children:"Hello! Welcome to a website dedicated to the modern Luddite. This term is not to be confused with someone against technology\u2013hopefully proven by my using a website to convey my thoughts on what Neo-Luddism stands for\u2013but, rather, a person who simply wants to prioritize ethics in the current technological shift humanity is experiencing. On the right, you will find an anthology I wrote that examines how the term Luddite has been misconstrued in history and why ensuring ethical standards are met in technological industries are more crucial to our world than ever before. For readers who may be looking for a short summary of the entire anthology, reading the first four paragraphs from the title \u201cThe Neverending Chase of Advancement: How Human Nature has led us to Manufacture a Divine Power\u201d to the first primary source under the header \u201cLuddism\u201d will be enough background information to take full advantage of the rest of this website. On the news tab, you will find articles that expand on the Data Religion and discuss the massive impact our actions, whether intentional or accidental, have and how they might jeopardize equity for individuals or large groups of people. Thank you, and, if you take anything away, please remember that Luddism has nothing to do with the rate of technological advancement, but with ensuring proper precautions and equitable practices are employed."})}),Object(h.jsx)("div",{className:"Doc",children:Object(h.jsx)(u.a,{allowtransparency:"true",src:"https://docs.google.com/document/d/e/2PACX-1vQBKJKOAI-EM2GfNLCml4ozkxl3POVAUk-shrFzMtIrVlNLvnWdyXHZNHG_P6REmObTUe2XUl1ufsQ_/pub?embedded=true",width:"900px",height:"1000px",id:"myId",className:"myClassname",display:"flex",text:!0,align:"right",position:"relative",frameborder:"10",style:"background: #7A00B2;"})})]})})},p=(i(45),function(){return Object(h.jsx)("div",{className:"container",children:Object(h.jsxs)("div",{class:"site-wrap",children:[Object(h.jsxs)("section",{"data-scroll":!0,children:[Object(h.jsx)("h1",{"data-splitting":!0,children:"Equity in the \u201cData Religion\u201d Era"}),Object(h.jsx)("p",{children:"Oftentimes, we, the general public, tend to mistake sophistication with accuracy and repetition with truth. Unfortunately, this practice becomes a huge fallacy in the human\u2019s modern imperative of uncovering the realities behind the systems we engage with on a daily basis. As Bill Foster, the leader of a webinar on Equitable Algorithms, points out: \u201cAI can inherently be biased.\u201d In fact, credit markets often reflect our nation\u2019s history of discrimination. According to Melissa Koida, 20% of US adults lack a sufficient credit history to be scored and another 30% struggle to access affordable credit because their scores are non-prime. Additionally, nearly 30% of African-Americans and Hispanics cannot be scored under traditional means, compared to 16% of whites and Asians. Lisa Rice further scrutinizes the lending and housing systems in place in America by unveiling that 40% of black adults in Detroit, Michigan are credit invisible. All of these statistics prove that current methodologies in place within financial institutions are unequitable and further discriminatory practices. As Bill Foster states, \u201cNo one should be denied the opportunity to own a home, a pillar of the American dream, because of a non-human automated and awful unlawfully discriminatory decision.\u201d However, making AI fair is much easier said than done. To truly initiate change in the procedures of these monetary establishments, Lisa Rice suggests \u201c Integrat[ing] review of racial and other forms of bias into every phase of the algorithm\u2019s life cycle, including data selection, development, deployment, and monitoring\u201d and ensuring \u201cAI stakeholders, including regulators, scientists, engineers, and more [are] trained on fair housing and fair lending issues.\u201d None of the panelists disagree with the notion that AI can greatly exceed our goals for equality, but all agree that achieving them will take a great level of cooperation between, our government, corporate enterprises, and the public. Stephen Hayes introduces the distinction between disparate treatment disparate impact, which are terms for the intentional and unintentional challenges caused for minorities due to unethical systems utilized by banking institutions at assessing creditworthiness and other metrics that are essential to living an enriching life. Hayes suggests that substituting \u201cvariables in the models with the goal of identifying variations of models that maintain performance but that have less disparate impact\u201d is beneficial for companies and consumers. Additionally, he states that more holistic efforts, such as \u201cfair lending training for modelers, ensuring teams have diverse backgrounds, and reviewing policies within which models operate\u201d are also helpful courses of action. He believes that companies do not want to further discrimination, but finds it difficult to locate machine learning models that can detect accurately. Kareem Saleh offers further insight into the aforementioned practice, suggesting that although this method of \u201ctaking credit scores out of an algorithm, rerunning it, and evaluating the differences in outcomes for protected groups\u201d may result in fairer outcomes, it is often less profitable for the company. These businesses have no guidance as to how they should handle the trade-off between profitability and equitability. Additionally, \u201clenders fear that the very act of trying to find a fairer better means of underwriting or pricing loans could be used against them as evidence they knew their algorithms were biased, to begin with.\u201d Surprisingly, there are models that can disentangle a credit score\u2019s predictive power from its disparity driving effects, which results in an increased approval rate for protected groups of about 10-30% without increasing risk for banks. He also mentions that policymakers can further assist by establishing goals or practices for balancing profitability with equitability. Overall, everybody agrees that AI models will have a large impact on the financial institutions within America, and everybody sees their extraordinary potential for good, but the challenge is ensuring that at every step of the process of advancing machines fair outcomes are prioritized."}),Object(h.jsx)("a",{href:"https://www.youtube.com/watch?v=t7EmCHL5bHs",children:"Video Conference"})]}),Object(h.jsxs)("section",{"data-scroll":!0,children:[Object(h.jsx)("h1",{"data-splitting":!0,children:"Technology-Promoted Racism"}),Object(h.jsx)("p",{children:"Dr. Safiya Umoja Noble was surprised when Google first emerged and people began utilizing it as a trusted resource when she reckoned it was really just an advertising platform. When she searched \u201cBlack girls\u201d on the website, Google returned pornography and hyper-sexualized content. Although Google began to suppress some of this content in 2012, Latina and Asian girls were still represented this way. In fact, a Markup study found that black girls, latina girls, and asian girls were profoundly linked with adult content while only \u201cwhite girls\u201d and \u201cwhite boys\u201d did not return pornographic keywords. A combination of hyperlinking, advertising and capital, and what people click on drives what we find on the web. Additionally, Google utilizes the data gathered from clicks on advertisements to repeatedly employ the same advertisements that garner the most attention. Consequently, many of the advertisements relate to pornographic material. One of the hardest stories for Dr. Safiya Umoja Noble to investigate was the Charleston church shooting. She learned that in trying to make sense of the trial of George Zimmerman, a neighborhood watchman in Florida who unjustifiably shot and killed 17-year-old Trayvon Martin, Dylann Roof stumbled across the Council of Conservative Citizens website, a deeply racist community. This website spreads disinformation, such as claiming that more Black people kill white people than white people do when, in reality, FBI data suggests 2,594 white people killed white people and 566 Black people killed white people. After traveling down a rabbit hole of white supremacy websites, Roof murders nine African-Americans and says he wants to start a race war. Dr. Latanya Sweeney was the first African-American to get a PhD in Computer Science at MIT. During a meeting, she typed in her name to Google search to pull up some information about herself. Instead, she and the person she was meeting with were startled by an advertisement that appeared which suggested she had a criminal record. Upon clicking on the advertisement, nobody with her name had any arrest records on the website. Pursuing this discovery further, Sweeney found that white-associated names most often resulted in neutral ad, while Black-associated names resulted in ads indicating an arrest record more than 80% of the time. She remarks that we do not have much policy over these systems, yet they dictate how we live."}),Object(h.jsx)("a",{href:"https://www.youtube.com/watch?v=9K9ZR_lGRpY",children:"Documentary"})]}),Object(h.jsxs)("section",{"data-scroll":!0,children:[Object(h.jsx)("h1",{"data-splitting":!0,children:"Identifying Unethical Uses of Tech"}),Object(h.jsx)("p",{children:"Worries about robots replacing humans in the workforce have been circulating for centuries. So far, data has shown that increases in automation result in more jobs. Projections from the WEF reiterate this data, suggesting that, although machines might eliminate 85 million occupations by 2025, 97 million jobs are expected to be created in that time period. While the declining rate of jobs may become much more significant in the future as the efficiency of Artificial Intelligence grows at an exponential rate, for now, there are more pressing ethical concerns around technological growth than the workplace. Not only are there the previous articles I have written that cover the systemic racism inherent in algorithms, but there are also newfound technologies that post just as significant ethical dilemmas. For example, CRISPR may allow people to not only cure mental and physical illnesses but give themselves preferable characteristics. According to UCSF, this future might be just 30 years away. Brain-machine interfaces and other implantable and external biotechnology also must be explored safely. Governments have been known to test futuristic technologies without much caution or care for human life, and a mistake in this new era of advancement could truly be catastrophic to all of humanity."}),Object(h.jsx)("a",{href:"https://magazine.ucsf.edu/technology-will-soon-give-us-precise-control-over-our-brains-and-genes",children:"UCSF Statistics"}),Object(h.jsx)("a",{href:"https://www.cnbc.com/2020/10/20/wef-says-machines-will-create-jobs-but-warns-of-pandemic-disruption.html",children:"WEF Job Data"})]})]})})}),b=(i(46),function(){return Object(h.jsxs)("div",{id:"news",children:[Object(h.jsx)("h1",{"data-splitting":!0,children:"Become a Luddite"}),Object(h.jsx)("p",{}),Object(h.jsxs)("ul",{id:"articles",children:[Object(h.jsx)("li",{class:"article-item",children:Object(h.jsxs)("h2",{children:[Object(h.jsx)("p",{children:"Guidelines for proper ethical practices for computer scientists looking to create AI algorithms!"}),Object(h.jsx)("a",{href:"https://github.com/EthicalML/awesome-artificial-intelligence-guidelines",children:"Ethical Practices for Computer Science"})]})}),Object(h.jsx)("li",{class:"article-item",children:Object(h.jsxs)("h2",{children:[Object(h.jsx)("p",{children:"Guidelines and work for becoming an Anti-Racist!"}),Object(h.jsx)("a",{href:"http://bit.ly/ANTIRACISMRESOURCES",children:"Becoming Anti-Racist"})]})}),Object(h.jsx)("li",{class:"article-item",children:Object(h.jsxs)("h2",{children:[Object(h.jsx)("p",{children:"Fascinating article further supporting the argument for Luddism (incase you were not convinced yet)!"}),Object(h.jsx)("a",{href:"https://theconversation.com/im-a-luddite-you-should-be-one-too-163172",children:"Support for Luddism"})]})})]})]})}),f=i(11),g=i(14),j=i.n(g),w=function(){var e=Object(a.useState)(""),t=Object(f.a)(e,2),i=t[0],n=t[1],s=Object(a.useState)(""),o=Object(f.a)(s,2),r=o[0],c=o[1],l=Object(a.useState)(""),d=Object(f.a)(l,2),u=d[0],m=d[1],p=Object(a.useState)(""),b=Object(f.a)(p,2),g=b[0],w=b[1],y=Object(a.useState)(!1),v=Object(f.a)(y,2),x=v[0],O=v[1];j.a.defaults.withCredentials=!0;return Object(h.jsxs)("div",{className:"App",children:[Object(h.jsxs)("div",{className:"registration",children:[Object(h.jsx)("h1",{children:"Registration"}),Object(h.jsx)("label",{children:"Username"}),Object(h.jsx)("input",{type:"text",onChange:function(e){n(e.target.value)}}),Object(h.jsx)("label",{children:"Password"}),Object(h.jsx)("input",{type:"password",onChange:function(e){c(e.target.value)}}),Object(h.jsx)("button",{onClick:function(){j.a.post("http://localhost:3002/register",{username:i,password:r}).then((function(e){console.log(e)}))},children:"Register"})]}),Object(h.jsxs)("div",{className:"login",children:[Object(h.jsx)("h1",{children:"Login"}),Object(h.jsx)("input",{type:"text",placeholder:"Username...",onChange:function(e){m(e.target.value)}}),Object(h.jsx)("input",{type:"password",placeholder:"Password...",onChange:function(e){w(e.target.value)}}),Object(h.jsx)("button",{onClick:function(){j.a.post("http://localhost:3002/login",{username:u,password:g}).then((function(e){e.data.error?O(!1):(console.log(e.data),O(!0))}))},children:"Login"})]}),x&&Object(h.jsx)("button",{onClick:function(){j.a.get("http://localhost:3002/profile").then((function(e){console.log(e)}))},children:"Check if Authenticated"})]})},y=function(){return Object(h.jsx)(o.a,{basename:"/DYOHistory.github.io",children:Object(h.jsxs)("div",{children:[Object(h.jsx)(d,{}),Object(h.jsxs)(r.c,{children:[Object(h.jsx)(r.a,{path:"/",component:m,exact:!0}),Object(h.jsx)(r.a,{path:"/news",component:p,exact:!0}),Object(h.jsx)(r.a,{path:"/support",component:b,exact:!0}),Object(h.jsx)(r.a,{path:"/profile",component:w,exact:!0}),Object(h.jsx)(r.a,{path:"*",component:m})]})]})})};s.a.render(Object(h.jsx)(y,{}),document.getElementById("root"))}},[[65,1,2]]]);
//# sourceMappingURL=main.70f72438.chunk.js.map